{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtuFhGtL8adztnvMCMA71Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeBlockFaiz/DATASCIENCE-Projects/blob/main/Mini_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Install packages"
      ],
      "metadata": {
        "id": "XXcQRSLLlIeM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwHSNnazlBBA",
        "outputId": "1f8d4e31-74c6-4aec-efb6-3189e07aac9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m112.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install streamlit pyngrok pandas numpy matplotlib scikit-learn torch statsmodels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Config + utils"
      ],
      "metadata": {
        "id": "ztQCkMBTlNq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, warnings\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ==== Paths ====\n",
        "ART_DIR = Path(\"imputed_outputs\")\n",
        "PLOT_DIR = Path(\"plots\")\n",
        "FORECAST_PLOT_DIR = Path(\"forecast_plots\")\n",
        "for d in [ART_DIR, PLOT_DIR, FORECAST_PLOT_DIR]:\n",
        "    d.mkdir(exist_ok=True)\n",
        "\n",
        "# ==== Your datasets ====\n",
        "DATASETS = {\n",
        "    \"Bangalore\": \"Bangalore 2023-08-08 to 2025-08-08.csv\",\n",
        "    \"Delhi\":     \"Delhi 2023-08-08 to 2025-08-08.csv\",\n",
        "}\n",
        "\n",
        "# Columns\n",
        "DATETIME_COL = \"datetime\"   # change if different in your CSVs\n",
        "FEATURES = [\"temp\", \"humidity\", \"windspeed\", \"winddir\", \"cloudcover\", \"dew\", \"precip\"]\n",
        "\n",
        "VAL_RATIO, TEST_RATIO = 0.1, 0.1\n",
        "\n",
        "def train_val_test_split(df, val_ratio=0.1, test_ratio=0.1, sort_col=\"datetime\"):\n",
        "    if sort_col in df.columns:\n",
        "        df = df.sort_values(sort_col)\n",
        "    n = len(df)\n",
        "    n_test = int(n * test_ratio)\n",
        "    n_val = int(n * val_ratio)\n",
        "    train = df.iloc[: n - n_val - n_test]\n",
        "    val   = df.iloc[n - n_val - n_test : n - n_test]\n",
        "    test  = df.iloc[n - n_test :]\n",
        "    return train.copy(), val.copy(), test.copy()\n",
        "\n",
        "def nrmse(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    rmse = np.sqrt(np.mean((y_true - y_pred)**2))\n",
        "    denom = (y_true.max() - y_true.min()) or 1.0\n",
        "    return float(rmse / denom)\n",
        "\n",
        "def nmse(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    mse = np.mean((y_true - y_pred)**2)\n",
        "    denom = np.mean((y_true - np.mean(y_true))**2) or 1.0\n",
        "    return float(mse / denom)"
      ],
      "metadata": {
        "id": "JyRPbBE1lRIm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Imputers (Mean/Median/KNN + CGAN)"
      ],
      "metadata": {
        "id": "L7qiGHQUlUQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# --- Simple statistical imputers ---\n",
        "def mean_impute(df, features):\n",
        "    out = df.copy()\n",
        "    for c in features:\n",
        "        out[c] = out[c].fillna(out[c].mean())\n",
        "    return out\n",
        "\n",
        "def median_impute(df, features):\n",
        "    out = df.copy()\n",
        "    for c in features:\n",
        "        out[c] = out[c].fillna(out[c].median())\n",
        "    return out\n",
        "\n",
        "def knn_impute(df, features, n_neighbors=5):\n",
        "    out = df.copy()\n",
        "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
        "    out[features] = imputer.fit_transform(out[features])\n",
        "    return out\n",
        "\n",
        "# --- CGAN Imputer (fixed/stable) ---\n",
        "class CGANImputer:\n",
        "    def __init__(self, features, noise_dim=8, hidden=64, lr=1e-3, treat_zeros_as_nan=True):\n",
        "        self.features = features\n",
        "        self.noise_dim = noise_dim\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.treat_zeros_as_nan = treat_zeros_as_nan\n",
        "\n",
        "        D_in = len(features)\n",
        "\n",
        "        class Gen(nn.Module):\n",
        "            def __init__(self, D_in, noise_dim, hidden):\n",
        "                super().__init__()\n",
        "                self.net = nn.Sequential(\n",
        "                    nn.Linear(D_in + noise_dim, hidden), nn.ReLU(),\n",
        "                    nn.Linear(hidden, hidden), nn.ReLU(),\n",
        "                    nn.Linear(hidden, D_in)\n",
        "                )\n",
        "            def forward(self, x, z):\n",
        "                return self.net(torch.cat([x, z], dim=1))\n",
        "\n",
        "        class Disc(nn.Module):\n",
        "            def __init__(self, D_in, hidden):\n",
        "                super().__init__()\n",
        "                self.net = nn.Sequential(\n",
        "                    nn.Linear(D_in * 2, hidden), nn.LeakyReLU(0.2),\n",
        "                    nn.Linear(hidden, hidden), nn.LeakyReLU(0.2),\n",
        "                    nn.Linear(hidden, 1), nn.Sigmoid()\n",
        "                )\n",
        "            def forward(self, x, x_hat):\n",
        "                return self.net(torch.cat([x, x_hat], dim=1))\n",
        "\n",
        "        self.G = Gen(D_in, noise_dim, hidden).to(self.device)\n",
        "        self.D = Disc(D_in, hidden).to(self.device)\n",
        "        self.g_opt = optim.Adam(self.G.parameters(), lr=lr)\n",
        "        self.d_opt = optim.Adam(self.D.parameters(), lr=lr)\n",
        "        self.mse = nn.MSELoss()\n",
        "\n",
        "    def _prepare_data(self, df):\n",
        "        X = df[self.features].values.astype(np.float32)\n",
        "        if self.treat_zeros_as_nan:\n",
        "            X[X == 0] = np.nan\n",
        "        M = ~np.isnan(X)\n",
        "\n",
        "        self.means = np.nanmean(X, axis=0, keepdims=True)\n",
        "        self.stds  = np.nanstd(X, axis=0, keepdims=True) + 1e-6\n",
        "\n",
        "        Xn = (X - self.means) / self.stds\n",
        "        Xn = np.nan_to_num(Xn, nan=0.0)\n",
        "        return Xn, M\n",
        "\n",
        "    def fit(self, df, epochs=200, batch_size=64, verbose=False):\n",
        "        Xn, M = self._prepare_data(df)\n",
        "        dl = torch.utils.data.DataLoader(\n",
        "            torch.utils.data.TensorDataset(\n",
        "                torch.tensor(Xn, dtype=torch.float32),\n",
        "                torch.tensor(M.astype(np.float32))\n",
        "            ), batch_size=batch_size, shuffle=True\n",
        "        )\n",
        "        for ep in range(epochs):\n",
        "            for xb, mb in dl:\n",
        "                xb, mb = xb.to(self.device), mb.to(self.device)\n",
        "                z = torch.randn((xb.size(0), self.noise_dim), device=self.device)\n",
        "\n",
        "                # Generator pass\n",
        "                x_hat = self.G(xb * mb, z)\n",
        "                x_tilde = mb * xb + (1 - mb) * x_hat\n",
        "\n",
        "                # Discriminator\n",
        "                self.d_opt.zero_grad()\n",
        "                real_score = self.D(xb, xb)\n",
        "                fake_score = self.D(xb, x_tilde.detach())\n",
        "                d_loss = -torch.mean(torch.log(real_score + 1e-6) + torch.log(1 - fake_score + 1e-6))\n",
        "                d_loss.backward()\n",
        "                self.d_opt.step()\n",
        "\n",
        "                # Generator\n",
        "                self.g_opt.zero_grad()\n",
        "                fake_score = self.D(xb, x_tilde)\n",
        "                adv_loss = -torch.mean(torch.log(fake_score + 1e-6))\n",
        "                recon_loss = self.mse((1 - mb) * x_hat, (1 - mb) * xb)\n",
        "                g_loss = adv_loss + recon_loss\n",
        "                g_loss.backward()\n",
        "                self.g_opt.step()\n",
        "\n",
        "            if verbose and (ep+1) % max(1, epochs//10) == 0:\n",
        "                print(f\"Epoch {ep+1}/{epochs} D:{d_loss.item():.4f} G:{g_loss.item():.4f}\")\n",
        "\n",
        "    def transform(self, df):\n",
        "        X = df[self.features].values.astype(np.float32)\n",
        "        if self.treat_zeros_as_nan:\n",
        "            X[X == 0] = np.nan\n",
        "        M = ~np.isnan(X)\n",
        "\n",
        "        Xn = (X - self.means) / self.stds\n",
        "        Xn = np.nan_to_num(Xn, nan=0.0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            xb = torch.tensor(Xn, device=self.device)\n",
        "            mb = torch.tensor(M.astype(np.float32), device=self.device)\n",
        "            z = torch.randn((xb.size(0), self.noise_dim), device=self.device)\n",
        "            x_hat = self.G(xb * mb, z)\n",
        "            x_tilde = mb * xb + (1 - mb) * x_hat\n",
        "\n",
        "        X_rec = x_tilde.cpu().numpy() * self.stds + self.means\n",
        "        out = df.copy()\n",
        "        out[self.features] = X_rec\n",
        "        return out"
      ],
      "metadata": {
        "id": "XbKvTfCBlZyR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Imputation loop (saves CSVs + plots with bigger imputed dots)"
      ],
      "metadata": {
        "id": "eB88VOyVldQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for city, filepath in DATASETS.items():\n",
        "    print(f\"\\n=== Processing {city} ===\")\n",
        "    df = pd.read_csv(filepath)\n",
        "\n",
        "    # Ensure datetime exists\n",
        "    if DATETIME_COL in df.columns:\n",
        "        df[DATETIME_COL] = pd.to_datetime(df[DATETIME_COL], errors=\"coerce\")\n",
        "    else:\n",
        "        # try best-effort autodetect\n",
        "        dt_col = [c for c in df.columns if \"date\" in c.lower() or \"time\" in c.lower()]\n",
        "        if dt_col:\n",
        "            df[DATETIME_COL] = pd.to_datetime(df[dt_col[0]], errors=\"coerce\")\n",
        "        else:\n",
        "            df[DATETIME_COL] = pd.date_range(\"2023-01-01\", periods=len(df), freq=\"D\")\n",
        "\n",
        "    # Keep only existing features\n",
        "    feat = [f for f in FEATURES if f in df.columns]\n",
        "    if not feat:\n",
        "        raise ValueError(f\"No expected features found in {filepath}. Found columns: {df.columns.tolist()}\")\n",
        "\n",
        "    print(\"NaNs before:\", df[feat].isna().sum().to_dict())\n",
        "\n",
        "    # Save baselines (optional)\n",
        "    mean_impute(df, feat).to_csv(ART_DIR / f\"{city}_imputed_mean.csv\", index=False)\n",
        "    median_impute(df, feat).to_csv(ART_DIR / f\"{city}_imputed_median.csv\", index=False)\n",
        "    if len(feat) >= 2:\n",
        "        knn_impute(df, feat, n_neighbors=5).to_csv(ART_DIR / f\"{city}_imputed_knn.csv\", index=False)\n",
        "\n",
        "    # CGAN imputation\n",
        "    cgan = CGANImputer(feat, treat_zeros_as_nan=True)\n",
        "    cgan.fit(df, epochs=200, verbose=True)\n",
        "    df_cgan = cgan.transform(df)\n",
        "    df_cgan.to_csv(ART_DIR / f\"{city}_imputed_cgan.csv\", index=False)\n",
        "\n",
        "    print(\"NaNs after (CGAN):\", df_cgan[feat].isna().sum().to_dict())\n",
        "\n",
        "    # Plots\n",
        "    time = df[DATETIME_COL]\n",
        "    for f in feat:\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        plt.plot(time, df[f], label=\"Original\", color=\"blue\", alpha=0.6)\n",
        "        miss = df[f].isna()\n",
        "        if miss.any():\n",
        "            plt.scatter(time[miss], [np.nanmean(df[f])] * miss.sum(), color=\"red\", marker=\"x\", s=60, label=\"Missing\")\n",
        "        plt.scatter(time, df_cgan[f], color=\"orange\", s=36, alpha=0.8, label=\"CGAN Imputed\")  # bigger dots\n",
        "        plt.title(f\"{city} ‚Äî {f} (CGAN Imputation)\")\n",
        "        plt.xlabel(\"Time\"); plt.ylabel(f)\n",
        "        plt.legend(); plt.tight_layout()\n",
        "        plt.savefig(PLOT_DIR / f\"{city}_{f}_cgan_imputation.png\", dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "print(\"\\n‚úÖ Imputed CSVs at:\", ART_DIR)\n",
        "print(\"‚úÖ Imputation plots at:\", PLOT_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEF6ccAmliP1",
        "outputId": "03982d7c-95af-4ddc-dcca-7bc230b08f08"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Processing Bangalore ===\n",
            "NaNs before: {'temp': 0, 'humidity': 0, 'windspeed': 0, 'winddir': 0, 'cloudcover': 0, 'dew': 0, 'precip': 0}\n",
            "Epoch 20/200 D:1.3863 G:0.6926\n",
            "Epoch 40/200 D:1.3863 G:0.6928\n",
            "Epoch 60/200 D:1.3863 G:0.6933\n",
            "Epoch 80/200 D:1.3863 G:0.6937\n",
            "Epoch 100/200 D:1.3863 G:0.6937\n",
            "Epoch 120/200 D:1.3863 G:0.6934\n",
            "Epoch 140/200 D:1.3863 G:0.6930\n",
            "Epoch 160/200 D:1.3863 G:0.6975\n",
            "Epoch 180/200 D:1.3863 G:0.6931\n",
            "Epoch 200/200 D:1.3863 G:0.6930\n",
            "NaNs after (CGAN): {'temp': 0, 'humidity': 0, 'windspeed': 0, 'winddir': 0, 'cloudcover': 0, 'dew': 0, 'precip': 0}\n",
            "\n",
            "=== Processing Delhi ===\n",
            "NaNs before: {'temp': 0, 'humidity': 0, 'windspeed': 0, 'winddir': 0, 'cloudcover': 0, 'dew': 0, 'precip': 0}\n",
            "Epoch 20/200 D:1.3857 G:0.6948\n",
            "Epoch 40/200 D:1.3863 G:0.6929\n",
            "Epoch 60/200 D:1.3863 G:0.6931\n",
            "Epoch 80/200 D:1.3863 G:0.6931\n",
            "Epoch 100/200 D:1.3863 G:0.6927\n",
            "Epoch 120/200 D:1.3863 G:0.6932\n",
            "Epoch 140/200 D:1.3863 G:0.6930\n",
            "Epoch 160/200 D:1.3863 G:0.6930\n",
            "Epoch 180/200 D:1.3863 G:0.6933\n",
            "Epoch 200/200 D:1.3863 G:0.6931\n",
            "NaNs after (CGAN): {'temp': 0, 'humidity': 0, 'windspeed': 0, 'winddir': 0, 'cloudcover': 0, 'dew': 0, 'precip': 0}\n",
            "\n",
            "‚úÖ Imputed CSVs at: imputed_outputs\n",
            "‚úÖ Imputation plots at: plots\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Bi-LSTM forecaster + training & saving predictions"
      ],
      "metadata": {
        "id": "MWpuC9bylm3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class BiLSTMForecaster:\n",
        "    def __init__(self, input_dim, hidden=64, layers=1, lr=1e-3):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model = nn.LSTM(input_dim, hidden, num_layers=layers, batch_first=True, bidirectional=True).to(self.device)\n",
        "        self.fc = nn.Linear(hidden*2, input_dim).to(self.device)\n",
        "        self.crit = nn.MSELoss()\n",
        "        self.opt = optim.Adam(list(self.model.parameters()) + list(self.fc.parameters()), lr=lr)\n",
        "\n",
        "    def _make_seq(self, df, features, seq_len):\n",
        "        X = df[features].values.astype(np.float32)\n",
        "        seqs, targs = [], []\n",
        "        for i in range(len(X) - seq_len):\n",
        "            seqs.append(X[i:i+seq_len])\n",
        "            targs.append(X[i+seq_len])\n",
        "        return np.array(seqs), np.array(targs)\n",
        "\n",
        "    def fit(self, df, features, seq_len=14, epochs=50, batch_size=64, verbose=False):\n",
        "        X, y = self._make_seq(df, features, seq_len)\n",
        "        X = torch.tensor(X, device=self.device)\n",
        "        y = torch.tensor(y, device=self.device)\n",
        "        for ep in range(1, epochs+1):\n",
        "            self.model.train()\n",
        "            perm = torch.randperm(len(X))\n",
        "            losses = []\n",
        "            for i in range(0, len(X), batch_size):\n",
        "                idx = perm[i:i+batch_size]\n",
        "                xb, yb = X[idx], y[idx]\n",
        "                out, _ = self.model(xb)\n",
        "                pred = self.fc(out[:, -1, :])\n",
        "                loss = self.crit(pred, yb)\n",
        "                self.opt.zero_grad(); loss.backward(); self.opt.step()\n",
        "                losses.append(loss.item())\n",
        "            if verbose and ep % 5 == 0:\n",
        "                print(f\"Epoch {ep}/{epochs} Loss: {np.mean(losses):.4f}\")\n",
        "\n",
        "    def forecast(self, hist_df, features, seq_len=14, steps=7):\n",
        "        X = hist_df[features].values.astype(np.float32)\n",
        "        seq = torch.tensor(X[-seq_len:], device=self.device).unsqueeze(0)\n",
        "        preds = []\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for _ in range(steps):\n",
        "                out, _ = self.model(seq)\n",
        "                pred = self.fc(out[:, -1, :])\n",
        "                preds.append(pred.cpu().numpy().ravel())\n",
        "                seq = torch.cat([seq[:, 1:, :], pred.unsqueeze(1)], dim=1)\n",
        "        return np.array(preds)\n",
        "\n",
        "# Train & save per city (on CGAN-imputed data)\n",
        "for city in DATASETS.keys():\n",
        "    print(f\"\\n=== Bi-LSTM training for {city} ===\")\n",
        "    dfc = pd.read_csv(ART_DIR / f\"{city}_imputed_cgan.csv\")\n",
        "    dfc[DATETIME_COL] = pd.to_datetime(dfc[DATETIME_COL], errors=\"coerce\")\n",
        "    dfc = dfc.sort_values(DATETIME_COL).reset_index(drop=True)\n",
        "\n",
        "    feat = [f for f in FEATURES if f in dfc.columns]\n",
        "    dfc[feat] = dfc[feat].replace([np.inf, -np.inf], np.nan).fillna(dfc[feat].mean())\n",
        "\n",
        "    train, val, test = train_val_test_split(dfc, val_ratio=VAL_RATIO, test_ratio=TEST_RATIO, sort_col=DATETIME_COL)\n",
        "\n",
        "    bl = BiLSTMForecaster(input_dim=len(feat), hidden=64, layers=1, lr=1e-3)\n",
        "    bl.fit(train, feat, seq_len=14, epochs=50, batch_size=64, verbose=True)\n",
        "\n",
        "    steps = len(test)\n",
        "    hist_df = pd.concat([train, val], ignore_index=True)\n",
        "    preds = bl.forecast(hist_df, feat, seq_len=14, steps=steps)\n",
        "\n",
        "    pred_df = pd.DataFrame(preds, columns=feat)\n",
        "    pred_df.insert(0, DATETIME_COL, test[DATETIME_COL].values)\n",
        "    pred_df.to_csv(ART_DIR / f\"{city}_bilstm_predictions.csv\", index=False)\n",
        "\n",
        "    # quick metrics\n",
        "    print(\"\\nAccuracy metrics:\")\n",
        "    for f in feat:\n",
        "        print(f\"{f:12}  nRMSE: {nrmse(test[f].values, pred_df[f].values):.4f}   nMSE: {nmse(test[f].values, pred_df[f].values):.4f}\")\n",
        "\n",
        "    # plots\n",
        "    for f in feat:\n",
        "        plt.figure(figsize=(10,4))\n",
        "        plt.plot(test[DATETIME_COL], test[f].values, label=\"Actual\", color=\"blue\")\n",
        "        plt.plot(test[DATETIME_COL], pred_df[f].values, label=\"Bi-LSTM Forecast\", color=\"orange\")\n",
        "        plt.title(f\"{city} ‚Äî {f} Forecast\"); plt.legend(); plt.tight_layout()\n",
        "        plt.savefig(FORECAST_PLOT_DIR / f\"{city}_{f}_forecast.png\", dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "print(\"\\n‚úÖ Bi-LSTM predictions saved in:\", ART_DIR)\n",
        "print(\"‚úÖ Forecast plots in:\", FORECAST_PLOT_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8URIQIglrp5",
        "outputId": "af42b813-4753-4ed7-db27-7a4ef8e04708"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Bi-LSTM training for Bangalore ===\n",
            "Epoch 5/50 Loss: 6479.7547\n",
            "Epoch 10/50 Loss: 5867.8168\n",
            "Epoch 15/50 Loss: 5333.9563\n",
            "Epoch 20/50 Loss: 4873.2355\n",
            "Epoch 25/50 Loss: 4438.6222\n",
            "Epoch 30/50 Loss: 4063.4376\n",
            "Epoch 35/50 Loss: 3748.4285\n",
            "Epoch 40/50 Loss: 3468.2416\n",
            "Epoch 45/50 Loss: 3223.9847\n",
            "Epoch 50/50 Loss: 3005.1152\n",
            "\n",
            "Accuracy metrics:\n",
            "temp          nRMSE: 3.8262   nMSE: 304.5817\n",
            "humidity      nRMSE: 1.6555   nMSE: 66.0948\n",
            "windspeed     nRMSE: 0.1205   nMSE: 1.1360\n",
            "winddir       nRMSE: 3.6133   nMSE: 441.7329\n",
            "cloudcover    nRMSE: 0.7596   nMSE: 7.7405\n",
            "dew           nRMSE: 6.9573   nMSE: 875.8728\n",
            "precip        nRMSE: 0.1712   nMSE: 1.0426\n",
            "\n",
            "=== Bi-LSTM training for Delhi ===\n",
            "Epoch 5/50 Loss: 8838.0828\n",
            "Epoch 10/50 Loss: 8215.4205\n",
            "Epoch 15/50 Loss: 7647.9555\n",
            "Epoch 20/50 Loss: 7200.7556\n",
            "Epoch 25/50 Loss: 6768.2432\n",
            "Epoch 30/50 Loss: 6388.7791\n",
            "Epoch 35/50 Loss: 6076.9098\n",
            "Epoch 40/50 Loss: 5790.2607\n",
            "Epoch 45/50 Loss: 5536.5592\n",
            "Epoch 50/50 Loss: 5281.9447\n",
            "\n",
            "Accuracy metrics:\n",
            "temp          nRMSE: 0.7055   nMSE: 9.7140\n",
            "humidity      nRMSE: 0.4393   nMSE: 4.1411\n",
            "windspeed     nRMSE: 0.1871   nMSE: 1.2042\n",
            "winddir       nRMSE: 0.4448   nMSE: 2.8668\n",
            "cloudcover    nRMSE: 0.3819   nMSE: 2.5065\n",
            "dew           nRMSE: 0.6910   nMSE: 10.7030\n",
            "precip        nRMSE: 0.1473   nMSE: 1.0065\n",
            "\n",
            "‚úÖ Bi-LSTM predictions saved in: imputed_outputs\n",
            "‚úÖ Forecast plots in: forecast_plots\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) Create the Streamlit dashboard (reads CGAN-imputed data and does live Bi-LSTM forecast to a user-selected end date)"
      ],
      "metadata": {
        "id": "78V65Mjulvej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dashboard_code = r\"\"\"\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from datetime import timedelta\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ART_DIR = Path(\"imputed_outputs\")\n",
        "DATETIME_COL = \"datetime\"\n",
        "FEATURES = [\"temp\",\"humidity\",\"windspeed\",\"winddir\",\"cloudcover\",\"dew\",\"precip\"]\n",
        "\n",
        "class BiLSTMForecaster:\n",
        "    def __init__(self, input_dim, hidden=64, layers=1, lr=1e-3):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model = nn.LSTM(input_dim, hidden, num_layers=layers, batch_first=True, bidirectional=True).to(self.device)\n",
        "        self.fc = nn.Linear(hidden*2, input_dim).to(self.device)\n",
        "        self.crit = nn.MSELoss()\n",
        "        self.opt = optim.Adam(list(self.model.parameters()) + list(self.fc.parameters()), lr=lr)\n",
        "\n",
        "    def fit(self, df, features, seq_len=14, epochs=50, batch_size=64, verbose=False):\n",
        "        X = df[features].values.astype(np.float32)\n",
        "        seqs, targs = [], []\n",
        "        for i in range(len(X)-seq_len):\n",
        "            seqs.append(X[i:i+seq_len]); targs.append(X[i+seq_len])\n",
        "        X_t = torch.tensor(np.array(seqs), device=self.device)\n",
        "        y_t = torch.tensor(np.array(targs), device=self.device)\n",
        "        for ep in range(1, epochs+1):\n",
        "            self.model.train()\n",
        "            perm = torch.randperm(len(X_t))\n",
        "            losses = []\n",
        "            for i in range(0, len(X_t), batch_size):\n",
        "                idx = perm[i:i+batch_size]\n",
        "                xb, yb = X_t[idx], y_t[idx]\n",
        "                out, _ = self.model(xb)\n",
        "                pred = self.fc(out[:, -1, :])\n",
        "                loss = self.crit(pred, yb)\n",
        "                self.opt.zero_grad(); loss.backward(); self.opt.step()\n",
        "                losses.append(loss.item())\n",
        "            if verbose and ep % 5 == 0:\n",
        "                print(f\"Epoch {ep}/{epochs} Loss: {np.mean(losses):.4f}\")\n",
        "\n",
        "    def forecast(self, hist_df, features, seq_len=14, steps=7):\n",
        "        X = hist_df[features].values.astype(np.float32)\n",
        "        seq = torch.tensor(X[-seq_len:], device=self.device).unsqueeze(0)\n",
        "        preds = []\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for _ in range(steps):\n",
        "                out, _ = self.model(seq)\n",
        "                pred = self.fc(out[:, -1, :])\n",
        "                preds.append(pred.cpu().numpy().ravel())\n",
        "                seq = torch.cat([seq[:, 1:, :], pred.unsqueeze(1)], dim=1)\n",
        "        return np.array(preds)\n",
        "\n",
        "st.title(\"üå§ Live Weather Forecast ‚Äî Bi-LSTM on CGAN-Imputed Data\")\n",
        "\n",
        "files = list(ART_DIR.glob(\"*_imputed_cgan.csv\"))\n",
        "cities = [f.stem.replace(\"_imputed_cgan\",\"\") for f in files]\n",
        "if not cities:\n",
        "    st.error(f\"No CGAN-imputed datasets in {ART_DIR}. Run the notebook blocks first.\")\n",
        "    st.stop()\n",
        "\n",
        "city = st.selectbox(\"City\", cities)\n",
        "df = pd.read_csv(ART_DIR / f\"{city}_imputed_cgan.csv\")\n",
        "df[DATETIME_COL] = pd.to_datetime(df[DATETIME_COL], errors=\"coerce\")\n",
        "df = df.sort_values(DATETIME_COL).reset_index(drop=True)\n",
        "\n",
        "# Clean\n",
        "df[FEATURES] = df[FEATURES].replace([np.inf, -np.inf], np.nan).fillna(df[FEATURES].mean())\n",
        "\n",
        "last_date = df[DATETIME_COL].max().date()\n",
        "st.caption(f\"Last historical date: {last_date}\")\n",
        "horizon = st.slider(\"Days to forecast beyond last date\", min_value=1, max_value=30, value=7)\n",
        "\n",
        "# Train & forecast live\n",
        "seq_len = 14\n",
        "bl = BiLSTMForecaster(input_dim=len(FEATURES), hidden=64, layers=1, lr=1e-3)\n",
        "bl.fit(df, FEATURES, seq_len=seq_len, epochs=40, batch_size=64, verbose=False)\n",
        "\n",
        "preds = bl.forecast(df, FEATURES, seq_len=seq_len, steps=horizon)\n",
        "future_dates = pd.date_range(df[DATETIME_COL].max() + pd.Timedelta(days=1), periods=horizon, freq=\"D\")\n",
        "pred_df = pd.DataFrame(preds, columns=FEATURES)\n",
        "pred_df.insert(0, DATETIME_COL, future_dates)\n",
        "\n",
        "st.subheader(f\"Forecast for {city}: next {horizon} day(s)\")\n",
        "st.dataframe(pred_df)\n",
        "\n",
        "# Plots: full history + forecast\n",
        "fig, axes = plt.subplots(len(FEATURES), 1, figsize=(10, len(FEATURES)*2.3), sharex=True)\n",
        "for i, feat in enumerate(FEATURES):\n",
        "    axes[i].plot(df[DATETIME_COL], df[feat], label=\"Historical\", alpha=0.6)\n",
        "    axes[i].plot(pred_df[DATETIME_COL], pred_df[feat], label=\"Forecast\", marker=\"o\")\n",
        "    axes[i].set_ylabel(feat); axes[i].grid(True, alpha=0.3)\n",
        "axes[0].legend()\n",
        "plt.tight_layout()\n",
        "st.pyplot(fig)\n",
        "\"\"\"\n",
        "with open(\"dashboard.py\", \"w\") as f:\n",
        "    f.write(dashboard_code)\n",
        "\n",
        "print(\"‚úÖ dashboard.py written\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1cNIQm1l1D8",
        "outputId": "f98a5ea8-33f5-4d83-b938-29e50743877e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ dashboard.py written\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7) Launch Streamlit in Colab via ngrok"
      ],
      "metadata": {
        "id": "jiYRiRHXl4fD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üöÄ Combined Colab cell: Start Streamlit + Ngrok in one go\n",
        "\n",
        "# 1Ô∏è‚É£ Install required packages\n",
        "!pip install -q streamlit pyngrok\n",
        "\n",
        "# 2Ô∏è‚É£ Import\n",
        "import subprocess, threading\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# 3Ô∏è‚É£ Set your NGROK_AUTH_TOKEN\n",
        "NGROK_AUTH_TOKEN = \"31EacTUezPiEa0iT3tRIeKlUzLY_3qfozLwY4pLMpgSEQBtDV\"  # <-- REPLACE THIS\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# 4Ô∏è‚É£ Function to run Streamlit\n",
        "def run_streamlit():\n",
        "    # Replace 'dashboard.py' with your actual dashboard file name\n",
        "    subprocess.run([\"streamlit\", \"run\", \"dashboard.py\", \"--server.port=8501\"])\n",
        "\n",
        "# 5Ô∏è‚É£ Start Streamlit in a separate thread\n",
        "threading.Thread(target=run_streamlit, daemon=True).start()\n",
        "\n",
        "# 6Ô∏è‚É£ Start ngrok tunnel\n",
        "ngrok.kill()  # ensure no old tunnels are running\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"üîó Open this URL to view the dashboard:\\n\", public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXlfDVPFojFz",
        "outputId": "1f2c1200-9a5e-4fa7-f225-173ae4fc6388"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîó Open this URL to view the dashboard:\n",
            " NgrokTunnel: \"https://f67fa65d0ce6.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}